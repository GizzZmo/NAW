/**
 * @fileoverview Web Audio API Engine for Playback Simulation
 * 
 * This service provides a basic audio synthesis engine using the browser's Web Audio API.
 * It simulates playback of the symbolic musical patterns generated by the Semantic Planner.
 * 
 * ## Current Implementation (Prototype)
 * 
 * - **Synthesis**: Basic oscillator-based sound design (kick, snare, bass, pads)
 * - **Purpose**: UI validation and workflow testing (NOT production audio quality)
 * - **Limitation**: No real neural vocoding (DAC/DisCoder) yet
 * 
 * ## Future Implementation
 * 
 * In the production system, this will be replaced by:
 * 1. **Neural Vocoder**: DisCoder or Vocos for high-fidelity waveform generation
 * 2. **Audio Streaming**: Load pre-rendered WAV files from neural models
 * 3. **Real-Time Effects**: EQ, compression, reverb using Web Audio API or VST
 * 4. **Low Latency**: ASIO/CoreAudio integration for <10ms round-trip
 * 
 * ## Architecture
 * 
 * ```
 * AudioEngine
 *   ├── Web Audio Context (master clock)
 *   ├── Master Gain (volume control)
 *   ├── Compressor (dynamics processing)
 *   └── Scheduler (timing engine)
 *       ├── Look-ahead scheduling (prevents glitches)
 *       └── Per-track event triggering
 * ```
 * 
 * @module services/audioEngine
 * @see {@link https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API|Web Audio API}
 * @see {@link https://www.html5rocks.com/en/tutorials/audio/scheduling/|Scheduling Guide}
 */

import { StemType, Track, Clip, NoteEvent } from '../types';

/**
 * Lookup table for converting note names to base frequencies (C0 reference).
 * 
 * Maps note names (including enharmonic equivalents) to their fundamental frequency
 * at octave 0. To get the actual frequency, multiply by 2^octave.
 * 
 * @constant
 * @type {Object.<string, number>}
 * @example
 * // Get A4 (440Hz)
 * const baseA = NOTE_TO_FREQ['A'];  // 27.50 Hz
 * const a4 = baseA * Math.pow(2, 4); // 440 Hz
 */
const NOTE_TO_FREQ: { [key: string]: number } = {
  'C': 16.35, 'C#': 17.32, 'Db': 17.32, 'D': 18.35, 'D#': 19.45, 'Eb': 19.45, 'E': 20.60,
  'F': 21.83, 'F#': 23.12, 'Gb': 23.12, 'G': 24.50, 'G#': 25.96, 'Ab': 25.96, 'A': 27.50,
  'A#': 29.14, 'Bb': 29.14, 'B': 30.87
};

/**
 * Converts a note value to frequency in Hz.
 * 
 * Supports multiple input formats:
 * - **Scientific Pitch Notation**: "C#4", "Gb3", "A0"
 * - **MIDI Number**: 60 (middle C), 69 (A4 = 440Hz)
 * - **Special**: "REST" or null → 0 Hz (silence)
 * 
 * @param {string|number} note - Note value in various formats
 * @returns {number} Frequency in Hz, or 0 for rest/invalid
 * 
 * @example
 * getFreq("A4")    // 440 Hz
 * getFreq("C#3")   // ~138.6 Hz
 * getFreq(60)      // 261.6 Hz (middle C)
 * getFreq("REST")  // 0 Hz
 */
const getFreq = (note: string | number): number => {
  if (typeof note === 'number') return note;
  if (!note || note === 'REST') return 0;
  
  // Parse Scientific Notation e.g. "C#4"
  const match = note.match(/^([A-G][#b]?)(-?\d+)$/);
  if (!match) return 440; // Default fallback
  
  const noteName = match[1];
  const octave = parseInt(match[2], 10);
  
  const baseFreq = NOTE_TO_FREQ[noteName];
  if (!baseFreq) return 440;
  
  // Formula: freq = base * 2^octave
  return baseFreq * Math.pow(2, octave);
};

/**
 * Audio Engine Class - Manages playback, scheduling, and synthesis.
 * 
 * This is a singleton pattern (exported instance at bottom of file).
 * It maintains the audio context, timing state, and handles real-time
 * scheduling of musical events from the track data.
 * 
 * ## Scheduling Algorithm
 * 
 * Uses a **look-ahead scheduler** to prevent audio dropouts:
 * 1. Timer runs every 25ms (lookahead)
 * 2. Schedules all events in the next 100ms window
 * 3. Audio nodes start at precise future times (sample-accurate)
 * 
 * This is superior to `setInterval` at exact BPM due to browser timer jitter.
 * 
 * @class AudioEngine
 */
class AudioEngine {
  /** Web Audio context (master clock and DSP graph root) */
  ctx: AudioContext | null = null;
  
  /** Master volume control (affects all tracks) */
  masterGain: GainNode | null = null;
  
  /** Master compressor (prevents clipping, adds "glue") */
  compressor: DynamicsCompressorNode | null = null;
  
  /** Current playback state */
  isPlaying: boolean = false;
  
  /** Tempo in beats per minute */
  bpm: number = 120;
  
  /** AudioContext time when playback started */
  startTime: number = 0;
  
  /** Bar position where playback started (for seeking) */
  startBar: number = 0;
  
  /** Look-ahead window duration in milliseconds */
  lookahead: number = 25.0;
  
  /** Schedule events this far into the future (seconds) */
  scheduleAheadTime: number = 0.1;
  
  /** Next scheduled event time (AudioContext time) */
  nextNoteTime: number = 0.0;
  
  /** Current step in the 16th-note grid */
  current16thNote: number = 0;
  
  /** Scheduler timer ID (for cancellation) */
  timerID: number | null = null;
  
  /** Reference to project tracks (updated from App.tsx) */
  tracks: Track[] = [];

  /**
   * Initializes the audio context.
   * 
   * Creates an AudioContext (cross-browser compatible) on construction.
   * The context may start in a "suspended" state due to browser autoplay policies.
   */
  constructor() {
    const AudioContextClass = (window as any).AudioContext || (window as any).webkitAudioContext;
    if (AudioContextClass) {
        this.ctx = new AudioContextClass();
    }
  }

  /**
   * Lazy initialization of audio graph components.
   * 
   * Creates master gain and compressor nodes on first playback.
   * Resumes context if suspended (handles autoplay restrictions).
   * 
   * **Audio Graph:**
   * ```
   * [Oscillators] → [Track Gains] → [Master Gain] → [Compressor] → [Destination]
   * ```
   */
  init() {
    if (!this.ctx) return;
    
    // Resume context if blocked by browser autoplay policy
    if (this.ctx.state === 'suspended') {
      this.ctx.resume();
    }
    
    // Create master chain (only once)
    if (!this.masterGain) {
        // Compressor settings (light mastering)
        this.compressor = this.ctx.createDynamicsCompressor();
        this.compressor.threshold.value = -18;  // Compress above -18dB
        this.compressor.ratio.value = 12;       // 12:1 ratio (hard limiting)
        this.compressor.connect(this.ctx.destination);
        
        // Master gain (prevents clipping before compressor)
        this.masterGain = this.ctx.createGain();
        this.masterGain.gain.value = 0.6;
        this.masterGain.connect(this.compressor);
    }
  }

  /**
   * Updates the track data reference.
   * 
   * Called from App.tsx whenever tracks change (new clips, edits, etc.)
   * 
   * @param {Track[]} tracks - Array of 4 tracks (DRUMS, BASS, VOCALS, OTHER)
   */
  setTracks(tracks: Track[]) {
    this.tracks = tracks;
  }

  /**
   * Updates the tempo.
   * 
   * @param {number} bpm - Beats per minute (60-240 typical range)
   */
  setBpm(bpm: number) {
    this.bpm = bpm;
  }

  /**
   * Calculates the current playhead position in bars.
   * 
   * Uses the AudioContext's high-precision clock to compute elapsed time
   * since playback started, then converts to bar units.
   * 
   * @returns {number} Current bar position (float for sub-bar precision)
   * @example
   * // If 2 bars have elapsed since start at bar 8
   * audioEngine.getCurrentBar(); // 10.0
   */
  getCurrentBar(): number {
      if (!this.ctx || !this.isPlaying) return this.startBar;
      
      // Time elapsed since start
      const elapsedTime = this.ctx.currentTime - this.startTime;
      
      // Convert to bars: (60/BPM) * 4 = seconds per bar in 4/4 time
      const secondsPerBar = (60 / this.bpm) * 4;
      
      return this.startBar + (elapsedTime / secondsPerBar);
  }

  /**
   * Starts playback from a specified bar position.
   * 
   * Initializes the audio context, sets up the scheduler, and begins
   * the look-ahead timing loop.
   * 
   * @param {number} [startBar=0] - Bar position to start from (supports seeking)
   */
  start(startBar: number = 0) {
    if (!this.ctx) return;
    this.init();
    if (this.isPlaying) return;

    this.isPlaying = true;
    this.startBar = startBar;
    this.startTime = this.ctx.currentTime;
    
    // Convert bar position to 16th-note step
    this.current16thNote = Math.floor(startBar * 16);
    
    // Schedule first event slightly in the future to prevent click
    this.nextNoteTime = this.ctx.currentTime + 0.05;
    
    this.scheduler();
  }

  /**
   * Stops playback and cancels the scheduler.
   */
  stop() {
    this.isPlaying = false;
    if (this.timerID) window.clearTimeout(this.timerID);
  }

  /**
   * Look-ahead scheduler loop.
   * 
   * This is the core timing engine. It runs every `lookahead` ms and schedules
   * all events that should trigger in the next `scheduleAheadTime` window.
   * 
   * **Why not setInterval at BPM?**
   * - Browser timers drift (10-20ms jitter)
   * - Look-ahead scheduling decouples timer precision from audio precision
   * - Audio nodes use AudioContext.currentTime (sample-accurate)
   * 
   * @private
   */
  scheduler() {
    if (!this.ctx) return;
    
    // Schedule all events in the lookahead window
    while (this.nextNoteTime < this.ctx.currentTime + this.scheduleAheadTime) {
      this.scheduleNote(this.current16thNote, this.nextNoteTime);
      this.advanceNote();
    }
    
    // Continue loop if still playing
    if (this.isPlaying) {
      this.timerID = window.setTimeout(() => this.scheduler(), this.lookahead);
    }
  }

  /**
   * Advances the playhead to the next 16th note.
   * 
   * Updates both the step counter and the next event time based on BPM.
   * 
   * @private
   */
  advanceNote() {
    // Calculate time between 16th notes
    const secondsPerBeat = 60.0 / this.bpm;
    this.nextNoteTime += 0.25 * secondsPerBeat; // 16th note = 1/4 of a beat
    this.current16thNote++;
  }

  /**
   * Schedules all events that trigger at a specific beat index.
   * 
   * Iterates through all tracks, finds active clips at the current bar,
   * and triggers any events that match the current step.
   * 
   * **Solo/Mute Logic:**
   * - If ANY track is solo'd, only solo'd tracks play
   * - Muted tracks never play
   * 
   * @param {number} beatIndex - Global 16th-note step (0 = bar 0 step 0)
   * @param {number} time - AudioContext time when event should trigger
   * @private
   */
  scheduleNote(beatIndex: number, time: number) {
    const bar = Math.floor(beatIndex / 16);
    
    this.tracks.forEach(track => {
      // Skip muted tracks
      if (track.muted) return;
      
      // Solo logic: if any track is solo'd, skip non-solo tracks
      if (this.tracks.some(t => t.solo) && !track.solo) return;

      if (!track.clips) return;

      // Find clip that's active at this bar
      const activeClip = track.clips.find(c => bar >= c.startBar && bar < c.startBar + c.lengthBars);
      
      if (activeClip) {
        this.playClipEvents(track, activeClip, time, beatIndex);
      }
    });
  }

  /**
   * Plays all events from a clip that trigger at the current step.
   * 
   * Handles looping: if the clip is 2 bars long, steps wrap around using modulo.
   * This allows short patterns to repeat seamlessly.
   * 
   * @param {Track} track - Parent track (for volume and stem type)
   * @param {Clip} clip - Clip containing events
   * @param {number} time - AudioContext trigger time
   * @param {number} globalBeatIndex - Global 16th-note step
   * @private
   */
  playClipEvents(track: Track, clip: Clip, time: number, globalBeatIndex: number) {
    if (!clip.events || clip.events.length === 0) return;

    // Calculate step relative to clip start
    const clipStartStep = clip.startBar * 16;
    const stepInClip = globalBeatIndex - clipStartStep;
    
    // Looping Logic: Modulo the step by the clip length (in steps)
    const loopLengthSteps = clip.lengthBars * 16;
    const currentLoopStep = stepInClip % loopLengthSteps;

    // Find events triggering on this specific step
    const events = clip.events.filter(e => e.step === currentLoopStep);

    events.forEach(event => {
       this.triggerEvent(track, event, time);
    });
  }

  /**
   * Triggers a single musical event (note or drum hit).
   * 
   * Routes to the appropriate synthesis function based on stem type.
   * Applies track volume and event velocity as gain scaling.
   * 
   * @param {Track} track - Parent track (determines synthesis type)
   * @param {NoteEvent} event - Event to trigger
   * @param {number} time - AudioContext trigger time
   * @private
   */
  triggerEvent(track: Track, event: NoteEvent, time: number) {
     if (!this.ctx || !this.masterGain) return;
     
     // Create per-event gain (velocity + track volume)
     const trackGain = this.ctx.createGain();
     trackGain.gain.value = track.volume * event.velocity;
     trackGain.connect(this.masterGain);
     
     // Route to stem-specific synthesis
     if (track.type === StemType.DRUMS) {
        const type = String(event.note).toUpperCase();
        if (type === 'KICK') this.playKick(time, trackGain);
        else if (type === 'SNARE' || type === 'CLAP') this.playSnare(time, trackGain);
        else if (type === 'CHAT') this.playHiHat(time, false, trackGain); // Closed
        else if (type === 'OHAT') this.playHiHat(time, true, trackGain);  // Open
     } 
     else if (track.type === StemType.BASS) {
        const freq = getFreq(event.note);
        this.playBass(time, freq, event.duration, trackGain);
     }
     else if (track.type === StemType.VOCALS) {
        const freq = getFreq(event.note);
        this.playVocalSynth(time, freq, event.duration, trackGain);
     }
     else if (track.type === StemType.OTHER) {
        const freq = getFreq(event.note);
        this.playPad(time, freq, event.duration, trackGain);
     }
  }

  // ═════════════════════════════════════════════════════════════════
  // SYNTHESIS IMPLEMENTATIONS (Basic Placeholder Sounds)
  // ═════════════════════════════════════════════════════════════════
  // These are intentionally simple. In production, these will be replaced
  // by neural vocoder output (DisCoder/Vocos rendering WAV files).

  /**
   * Synthesizes a kick drum sound.
   * 
   * Uses a pitch-falling sine wave (150Hz → 0Hz) to simulate bass drum resonance.
   * 
   * @param {number} time - AudioContext start time
   * @param {AudioNode} dest - Destination node (typically track gain)
   * @private
   */
  playKick(time: number, dest: AudioNode) {
     if (!this.ctx) return;
     const osc = this.ctx.createOscillator();
     const gain = this.ctx.createGain();
     osc.connect(gain);
     gain.connect(dest);
     
     // Pitch envelope: 150Hz → near-silence
     osc.frequency.setValueAtTime(150, time);
     osc.frequency.exponentialRampToValueAtTime(0.01, time + 0.5);
     
     // Amplitude envelope
     gain.gain.setValueAtTime(1, time);
     gain.gain.exponentialRampToValueAtTime(0.01, time + 0.5);
     
     osc.start(time);
     osc.stop(time + 0.5);
  }

  /**
   * Synthesizes a snare drum sound.
   * 
   * Combines white noise (filtered) with a tonal component for body.
   * 
   * @param {number} time - AudioContext start time
   * @param {AudioNode} dest - Destination node
   * @private
   */
  playSnare(time: number, dest: AudioNode) {
     if (!this.ctx) return;
     
     // Noise component (snare wires)
     const bufferSize = this.ctx.sampleRate * 0.2;
     const buffer = this.ctx.createBuffer(1, bufferSize, this.ctx.sampleRate);
     const data = buffer.getChannelData(0);
     for (let i = 0; i < bufferSize; i++) {
       data[i] = (Math.random() * 2 - 1);
     }
     
     const noise = this.ctx.createBufferSource();
     noise.buffer = buffer;
     const filter = this.ctx.createBiquadFilter();
     filter.type = 'highpass';
     filter.frequency.value = 800;
     const gain = this.ctx.createGain();
     
     noise.connect(filter);
     filter.connect(gain);
     gain.connect(dest);

     gain.gain.setValueAtTime(0.7, time);
     gain.gain.exponentialRampToValueAtTime(0.01, time + 0.2);
     noise.start(time);
     noise.stop(time + 0.2);
     
     // Tonal component (snare body)
     const osc = this.ctx.createOscillator();
     osc.type = 'triangle';
     const oscGain = this.ctx.createGain();
     osc.connect(oscGain);
     oscGain.connect(dest);
     osc.frequency.setValueAtTime(180, time);
     osc.frequency.exponentialRampToValueAtTime(100, time + 0.1);
     oscGain.gain.setValueAtTime(0.5, time);
     oscGain.gain.exponentialRampToValueAtTime(0.01, time + 0.1);
     osc.start(time);
     osc.stop(time + 0.1);
  }
  
  /**
   * Synthesizes a hi-hat sound (closed or open).
   * 
   * Uses filtered white noise with different decay times.
   * 
   * @param {number} time - AudioContext start time
   * @param {boolean} open - If true, longer decay (open hi-hat)
   * @param {AudioNode} dest - Destination node
   * @private
   */
  playHiHat(time: number, open: boolean, dest: AudioNode) {
     if (!this.ctx) return;
     const decay = open ? 0.3 : 0.05;
     const bufferSize = this.ctx.sampleRate * decay;
     const buffer = this.ctx.createBuffer(1, bufferSize, this.ctx.sampleRate);
     const data = buffer.getChannelData(0);
     for (let i = 0; i < bufferSize; i++) {
       data[i] = (Math.random() * 2 - 1);
     }
     const noise = this.ctx.createBufferSource();
     noise.buffer = buffer;
     
     const filter = this.ctx.createBiquadFilter();
     filter.type = 'highpass';
     filter.frequency.value = 8000;
     
     const gain = this.ctx.createGain();
     
     noise.connect(filter);
     filter.connect(gain);
     gain.connect(dest);
     
     gain.gain.setValueAtTime(0.6, time);
     gain.gain.exponentialRampToValueAtTime(0.001, time + decay);
     noise.start(time);
     noise.stop(time + decay);
  }

  /**
   * Synthesizes a bass sound.
   * 
   * Uses a sawtooth wave with resonant lowpass filter (303-style acid bass).
   * 
   * @param {number} time - AudioContext start time
   * @param {number} freq - Frequency in Hz
   * @param {number} durationSteps - Duration in 16th-note steps
   * @param {AudioNode} dest - Destination node
   * @private
   */
  playBass(time: number, freq: number, durationSteps: number, dest: AudioNode) {
    if (!this.ctx) return;
    const dur = (durationSteps * 0.25 * (60/this.bpm));
    
    const osc = this.ctx.createOscillator();
    osc.type = 'sawtooth';
    const filter = this.ctx.createBiquadFilter();
    filter.type = 'lowpass';
    const gain = this.ctx.createGain();
    
    osc.connect(filter);
    filter.connect(gain);
    gain.connect(dest);
    
    osc.frequency.value = freq;
    
    // Acid Envelope (filter cutoff automation)
    filter.frequency.setValueAtTime(300, time);
    filter.frequency.exponentialRampToValueAtTime(3000, time + 0.05);
    filter.frequency.exponentialRampToValueAtTime(300, time + 0.3);
    filter.Q.value = 5;
    
    gain.gain.setValueAtTime(1, time);
    gain.gain.setTargetAtTime(0, time + dur - 0.05, 0.05);
    
    osc.start(time);
    osc.stop(time + dur);
  }

  /**
   * Synthesizes a vocal-like sound.
   * 
   * Uses triangle wave with bandpass filter (formant approximation) and vibrato.
   * 
   * @param {number} time - AudioContext start time
   * @param {number} freq - Frequency in Hz
   * @param {number} durationSteps - Duration in 16th-note steps
   * @param {AudioNode} dest - Destination node
   * @private
   */
  playVocalSynth(time: number, freq: number, durationSteps: number, dest: AudioNode) {
     if (!this.ctx) return;
     const dur = (durationSteps * 0.25 * (60/this.bpm));
     const osc = this.ctx.createOscillator();
     osc.type = 'triangle';
     const filter = this.ctx.createBiquadFilter();
     filter.type = 'bandpass'; // Formant-ish
     filter.frequency.value = 1200; 
     filter.Q.value = 1;

     const gain = this.ctx.createGain();
     
     osc.connect(filter);
     filter.connect(gain);
     gain.connect(dest);
     
     osc.frequency.value = freq;
     // Vibrato (LFO modulating pitch)
     const lfo = this.ctx.createOscillator();
     lfo.frequency.value = 5;
     const lfoGain = this.ctx.createGain();
     lfoGain.gain.value = 5;
     lfo.connect(lfoGain);
     lfoGain.connect(osc.frequency);
     lfo.start(time);
     lfo.stop(time + dur);

     gain.gain.setValueAtTime(0, time);
     gain.gain.linearRampToValueAtTime(0.4, time + 0.05);
     gain.gain.linearRampToValueAtTime(0, time + dur);
     
     osc.start(time);
     osc.stop(time + dur);
  }

  /**
   * Synthesizes a pad/atmosphere sound.
   * 
   * Uses two detuned oscillators for chorus effect.
   * 
   * @param {number} time - AudioContext start time
   * @param {number} freq - Frequency in Hz
   * @param {number} durationSteps - Duration in 16th-note steps
   * @param {AudioNode} dest - Destination node
   * @private
   */
  playPad(time: number, freq: number, durationSteps: number, dest: AudioNode) {
     if (!this.ctx) return;
     const dur = (durationSteps * 0.25 * (60/this.bpm));
     const osc1 = this.ctx.createOscillator();
     const osc2 = this.ctx.createOscillator();
     osc1.type = 'sine';
     osc2.type = 'triangle';
     osc2.detune.value = 15; // Slight detune for width
     
     const gain = this.ctx.createGain();
     osc1.connect(gain);
     osc2.connect(gain);
     gain.connect(dest);
     
     osc1.frequency.value = freq;
     osc2.frequency.value = freq;

     gain.gain.setValueAtTime(0, time);
     gain.gain.linearRampToValueAtTime(0.2, time + 0.2);
     gain.gain.setTargetAtTime(0, time + dur - 0.2, 0.2);
     
     osc1.start(time);
     osc1.stop(time + dur + 0.5);
     osc2.start(time);
     osc2.stop(time + dur + 0.5);
  }
}

/**
 * Singleton instance of AudioEngine.
 * 
 * Exported for use throughout the application. Initialized on first import.
 * 
 * @type {AudioEngine}
 * @example
 * import { audioEngine } from './services/audioEngine';
 * audioEngine.start(0);
 */
export const audioEngine = new AudioEngine();
